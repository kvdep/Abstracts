---
aliases:
  - SVD-разложение
  - сингулярное разложение
  - SVD
  - Singular Value Decomposition
---
# Сингулярное разложение (SVD)

**Сингулярное разложение (SVD)** — это представление произвольной прямоугольной матрицы $A$ размера $m \times n$ в виде произведения трёх матриц:
$$
A = U \Sigma V^T
$$
где:
*   $U$ — ортогональная матрица размера $m \times m$, столбцы которой называются **левыми сингулярными векторами**.
*   $\Sigma$ — диагональная матрица размера $m \times n$, на диагонали которой расположены **сингулярные числа** $\sigma_i \ge 0$.
*   $V^T$ — транспонированная ортогональная матрица размера $n \times n$. Столбцы матрицы $V$ (строки $V^T$) называются **правыми сингулярными векторами**.

SVD существует для **любой** матрицы и является одним из самых важных инструментов в линейной алгебре и её приложениях.

## Основная идея и свойства

Геометрически SVD можно интерпретировать как разложение любого линейного преобразования на три последовательных действия:
1.  **Вращение** (описываемое матрицей $V^T$).
2.  **Масштабирование** вдоль осей (описываемое матрицей $\Sigma$).
3.  **Ещё одно вращение** (описываемое матрицей $U$).

### Свойства:
*   **Сингулярные числа** $\sigma_i$ на диагонали $\Sigma$ упорядочены по убыванию ($\sigma_1 \ge \sigma_2 \ge \dots \ge 0$). Они являются квадратными корнями из собственных значений матриц $A^T A$ и $A A^T$.
*   **Ранг матрицы** $A$ равен количеству её ненулевых сингулярных чисел.
*   **Число обусловленности** матрицы $A$ равно отношению максимального сингулярного числа к минимальному: $\text{cond}(A) = \frac{\sigma_1}{\sigma_{\min}}$.
*   **Теорема Эккарта-Янга:** Наилучшее приближение матрицы $A$ матрицей $A_k$ ранга $k$ (в смысле нормы Фробениуса) получается путём зануления всех сингулярных чисел, кроме первых $k$ наибольших.
    $$ A_k = U_k \Sigma_k V_k^T $$
    Это свойство лежит в основе сжатия данных с помощью SVD.

## Применение

### 1. Решение СЛАУ и метода наименьших квадратов
SVD предоставляет самый надёжный численный метод для решения [[Метод наименьших квадратов|задачи наименьших квадратов]], особенно для вырожденных или плохо обусловленных матриц. Решение системы $Ax=b$ через SVD находится с помощью **псевдообратной матрицы Мура-Пенроуза** $A^+$:
$$
x = A^+ b = V \Sigma^+ U^T b
$$
где $\Sigma^+$ получается из $\Sigma$ путём транспонирования и взятия обратных значений для ненулевых диагональных элементов.

### 2. Понижение размерности и сжатие данных
Благодаря теореме Эккарта-Янга, SVD является ядром **метода главных компонент ([[Метод главных компонент (PCA)|PCA]])**. Отбрасывая сингулярные векторы, соответствующие малым сингулярным числам, можно получить низкоранговое приближение исходных данных, сохранив при этом [[Необходимые условия максимума|максимум]] информации. Это широко используется для сжатия изображений, видео и других данных.

### 3. Вычисление ранга и определителя
SVD — самый точный способ численного определения ранга матрицы. Ранг равен числу сингулярных чисел, которые значительно больше машинного эпсилон.

## Сложность

Вычисление SVD — более затратная операция по сравнению с [[LU-разложение|LU]] или [[QR-разложение|QR-разложениями]]. Сложность для матрицы $m \times n$ составляет примерно $O(\min(m n^2, m^2 n))$.

## Пример на Python (сжатие изображения)

Классический пример, демонстрирующий мощь SVD, — сжатие изображений.

```python
import numpy as np
import matplotlib.pyplot as plt
from skimage.data import camera
from skimage.color import rgb2gray

def compress_image_svd(image, k):
    """
    Сжимает изображение с помощью SVD, оставляя k сингулярных чисел.

    Args:
        image (np.ndarray): Исходное изображение (2D массив).
        k (int): Количество оставляемых сингулярных чисел.

    Returns:
        np.ndarray: Сжатое (аппроксимированное) изображение.
    """
    # 1. Выполняем SVD
    U, s, Vt = np.linalg.svd(image, full_matrices=False)
    
    # 2. Обнуляем все сингулярные числа, кроме первых k
    s_k = np.zeros_like(s)
    s_k[:k] = s[:k]
    
    # 3. Восстанавливаем изображение
    # s_k - это вектор, поэтому для матричного умножения создаем диагональную матрицу
    compressed_image = U @ np.diag(s_k) @ Vt
    
    return compressed_image

if __name__ == '__main__':
    # Загружаем стандартное изображение и преобразуем в оттенки серого
    image = rgb2gray(camera())

    # Количество сингулярных чисел для аппроксимации
    k_values = [20, 50, 100]
    
    plt.figure(figsize=(16, 5))

    # Исходное изображение
    plt.subplot(1, len(k_values) + 1, 1)
    plt.imshow(image, cmap='gray')
    plt.title(f'Original\nRank = {np.linalg.matrix_rank(image)}')
    plt.axis('off')

    # Сжатые изображения
    for i, k in enumerate(k_values):
        compressed = compress_image_svd(image, k)
        
        plt.subplot(1, len(k_values) + 1, i + 2)
        plt.imshow(compressed, cmap='gray')
        plt.title(f'Compressed (k={k})')
        plt.axis('off')

    plt.suptitle('Image Compression using SVD', fontsize=16)
    plt.tight_layout(rect=[0, 0.03, 1, 0.95])
    plt.show()

    # Сравнение размеров
    m, n = image.shape
    original_size = m * n
    
    k = 50
    # Храним U_k (m*k), s_k (k), V_k (k*n)
    compressed_size = m * k + k + k * n
    
    print(f"Размер исходных данных: {original_size} чисел")
    print(f"Размер сжатых данных (k={k}): {compressed_size} чисел")
    print(f"Степень сжатия: {original_size / compressed_size:.2f} раз")

```