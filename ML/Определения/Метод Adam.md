---
aliases:
  - метод Adam
  - Adam
  - Adaptive Moment Estimation
---
# Метод Adam

**Метод Adam (Adaptive Moment Estimation)** — это адаптивный метод оптимизации, который вычисляет индивидуальные скорости обучения для разных параметров. Он является одним из самых популярных и эффективных методов, используемых в глубоком обучении.

Adam объединяет в себе две идеи:
1.  **Momentum (Инерция)**: Использование экспоненциально взвешенного скользящего среднего [[Градиент|градиентов]] (первый момент), чтобы ускорить сходимость и сгладить колебания.
2.  **RMSprop**: Использование экспоненциально взвешенного скользящего среднего квадратов [[Градиент|градиентов]] (второй момент), чтобы адаптировать скорость обучения для каждого параметра.

## Основная идея

На каждой итерации $t$ метод Adam обновляет параметры $x$, используя оценки первого и второго моментов [[Градиент|градиента]].

1.  **Оценка первого момента (среднее)**:
    $$
    m_t = \beta_1 m_{t-1} + (1 - \beta_1) g_t
    $$
    где $g_t = \nabla f(x^{(t-1)})$ — [[Градиент|градиент]] на текущем шаге, а $\beta_1$ — коэффициент затухания для первого момента.

2.  **Оценка второго момента (нецентрированная дисперсия)**:
    $$
    v_t = \beta_2 v_{t-1} + (1 - \beta_2) g_t^2
    $$
    где $g_t^2$ — поэлементный квадрат [[Градиент|градиента]], а $\beta_2$ — коэффициент затухания для второго момента.

3.  **Коррекция смещения (Bias correction)**:
    Поскольку $m_0$ и $v_0$ инициализируются нулями, на первых шагах оценки $m_t$ и $v_t$ смещены к нулю. Для коррекции этого смещения вводятся скорректированные оценки:
    $$
    \hat{m}_t = \frac{m_t}{1 - \beta_1^t}
    $$
    $$
    \hat{v}_t = \frac{v_t}{1 - \beta_2^t}
    $$

4.  **Обновление параметров**:
    Итоговая формула обновления параметров выглядит следующим образом:
    $$
    x^{(t)} = x^{(t-1)} - \lambda \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon}
    $$
    где $\lambda$ — скорость обучения, а $\epsilon$ — малая константа для предотвращения деления на ноль.

## Алгоритм

### Входные данные и гиперпараметры
*   $\lambda$: Скорость обучения (learning rate).
*   $\beta_1, \beta_2$: Коэффициенты затухания для моментов (обычно $\beta_1=0.9, \beta_2=0.999$).
*   $\epsilon$: Малая константа (обычно $10^{-8}$).
*   $x^{(0)}$: Начальные значения параметров.

### Псевдокод

```
// Инициализация
m = 0  // Инициализация первого момента
v = 0  // Инициализация второго момента
t = 0  // Инициализация счетчика шагов

while критерий остановки не выполнен:
    t = t + 1
    
    // Вычислить градиент
    g = ∇f(x_current)
    
    // Обновить моменты
    m = β1 * m + (1 - β1) * g
    v = β2 * v + (1 - β2) * g^2
    
    // Скорректировать смещение
    m_hat = m / (1 - β1^t)
    v_hat = v / (1 - β2^t)
    
    // Обновить параметры
    x_current = x_current - λ * m_hat / (sqrt(v_hat) + ε)

return x_current
```

## Преимущества

1.  **Эффективность**: Adam хорошо работает на практике и часто сходится быстрее, чем другие методы, такие как [[Стохастический градиентный спуск (SGD)]].
2.  **Адаптивная скорость обучения**: Для каждого параметра подбирается своя скорость обучения, что полезно для задач с разреженными [[Градиент|градиентами]] или "овражистыми" функциями потерь.
3.  **Малые требования к памяти**: Алгоритм требует немного дополнительной памяти для хранения моментов $m$ и $v$.
4.  **Устойчивость к выбору гиперпараметров**: Adam менее чувствителен к выбору начальной скорости обучения $\lambda$ по сравнению с [[Метод градиентного спуска|обычным градиентным спуском]].
