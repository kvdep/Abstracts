---
aliases:
  - PCA
  - метод главных компонент
  - Principal Component Analysis
---
# Метод главных компонент (PCA)

**Метод главных компонент (Principal Component Analysis, PCA)** — это один из самых распространённых методов **понижения размерности** данных без учителя (unsupervised learning). Его основная цель — преобразовать исходный набор признаков в новый, меньший по размеру набор, сохранив при этом как можно больше информации (дисперсии) из исходных данных.

Новые признаки, называемые **главными компонентами**, являются линейными комбинациями исходных признаков и ортогональны друг другу.

## Основная идея

Геометрически PCA находит новое ортогональное координатное пространство, оси которого (главные компоненты) направлены вдоль направлений максимальной дисперсии данных.

*   **Первая главная компонента** — это направление, вдоль которого данные имеют наибольшую дисперсию.
*   **Вторая главная компонента** — это направление, ортогональное первому, вдоль которого данные имеют наибольшую из оставшихся дисперсий.
*   И так далее.

Проецируя данные на подпространство, образованное первыми $k$ главными компонентами, мы получаем низкоразмерное представление данных, которое наилучшим образом сохраняет их "разброс".

## Алгоритм

1.  **Шаг 1: Стандартизация (или центрирование) данных.**
    *   Из каждого признака (столбца) вычитается его среднее значение.
    *   Часто признаки также делят на их стандартное отклонение, чтобы привести их к одному масштабу. Это важно, если признаки имеют разные единицы измерения.

2.  **Шаг 2: Вычисление ковариационной матрицы.**
    *   Для центрированной матрицы данных $X$ вычисляется ковариационная матрица:
        $$ C = \frac{1}{n-1} X^T X $$
    *   Ковариационная матрица показывает, как признаки изменяются совместно.

3.  **Шаг 3: Спектральное разложение ковариационной матрицы.**
    *   Находятся собственные векторы и собственные значения ковариационной матрицы $C$.
    *   Собственные векторы определяют направления главных компонент.
    *   Собственные значения показывают, какая доля дисперсии приходится на каждую компоненту.

4.  **Шаг 4: Выбор главных компонент.**
    *   Собственные векторы сортируются в порядке убывания соответствующих им собственных значений.
    *   Выбираются первые $k$ собственных векторов, где $k$ — желаемая размерность нового пространства.

5.  **Шаг 5: Проецирование данных.**
    *   Исходные (центрированные) данные проецируются на подпространство, образованное выбранными $k$ собственными векторами.

### Связь с SVD

На практике PCA почти всегда реализуется через [[SVD-разложение|SVD-разложение]] центрированной матрицы данных $X = U \Sigma V^T$.
*   **Правые сингулярные векторы** (столбцы матрицы $V$) являются главными компонентами.
*   **Сингулярные числа** (диагональные элементы $\Sigma$) связаны с собственными значениями ковариационной матрицы и показывают "важность" компонент.

Этот подход численно более устойчив, чем прямое вычисление ковариационной матрицы и её спектральное разложение.

## Применение

*   **Визуализация данных:** Снижение размерности до 2D или 3D для построения графиков.
*   **Повышение производительности моделей:** Уменьшение числа признаков ускоряет обучение моделей и может снизить риск переобучения.
*   **Сжатие данных:** Как и в [[SVD-разложение|SVD]], PCA можно использовать для сжатия данных с контролируемой потерей информации.
*   **Устранение мультиколлинеарности:** Главные компоненты ортогональны, что решает проблему линейной зависимости между исходными признаками.

## Пример на Python (с использованием Scikit-learn)

Используем классический набор данных "Ирисы Фишера" для визуализации.

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from sklearn.datasets import load_iris

def pca_visualization():
    """
    Демонстрация PCA на наборе данных "Ирисы Фишера".
    Снижение размерности с 4 до 2 для визуализации.
    """
    # 1. Загрузка данных
    iris = load_iris()
    X = iris.data  # 4 признака
    y = iris.target # 3 класса ирисов
    target_names = iris.target_names

    # 2. Стандартизация данных
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)

    # 3. Применение PCA
    # Указываем, что хотим получить 2 главные компоненты
    pca = PCA(n_components=2)
    X_pca = pca.fit_transform(X_scaled)

    # 4. Визуализация результатов
    plt.figure(figsize=(10, 7))
    colors = ['navy', 'turquoise', 'darkorange']
    lw = 2

    for color, i, target_name in zip(colors, [0, 1, 2], target_names):
        plt.scatter(X_pca[y == i, 0], X_pca[y == i, 1], color=color, alpha=.8, lw=lw,
                    label=target_name)
    
    plt.legend(loc='best', shadow=False, scatterpoints=1)
    plt.title('PCA на наборе данных "Ирисы Фишера"')
    plt.xlabel('Первая главная компонента')
    plt.ylabel('Вторая главная компонента')
    plt.grid(True)

    # Объясненная дисперсия
    explained_variance_ratio = pca.explained_variance_ratio_
    print(f"Объясненная дисперсия первыми двумя компонентами: "
          f"{sum(explained_variance_ratio) * 100:.2f}%")
    print(f"  - Первая компонента: {explained_variance_ratio[0] * 100:.2f}%")
    print(f"  - Вторая компонента: {explained_variance_ratio[1] * 100:.2f}%")

    plt.show()

if __name__ == '__main__':
    pca_visualization()

```