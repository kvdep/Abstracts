---
aliases:
  - МНК
  - метод наименьших квадратов
  - Method of Least Squares
  - OLS
  - Ordinary Least Squares
---
# Метод наименьших квадратов

**Метод наименьших квадратов (МНК)** — это фундаментальный метод математической статистики и численного анализа, применяемый для решения переопределённых [[Система линейных алгебраических уравнений|систем уравнений]] (когда уравнений больше, чем неизвестных) и для поиска аппроксимирующей функции для набора данных.

Основная идея — найти такие параметры модели, при которых сумма квадратов отклонений (ошибок, невязок) между наблюдаемыми данными и значениями, предсказанными моделью, была бы минимальной.

## Постановка задачи

Рассмотрим [[Система линейных алгебраических уравнений|СЛАУ]] $Ax = b$, где $A$ — матрица размера $m \times n$, а $b$ — вектор-столбец длины $m$. Часто на практике система является **переопределённой**, то есть $m > n$. Такая система, как правило, не имеет точного решения.

Вместо точного решения ищут такой вектор $x$, который минимизирует "ошибку" или **невязку** $r = b - Ax$. МНК требует минимизировать сумму квадратов компонент вектора невязки, то есть квадрат его евклидовой нормы:
$$
S(x) = \|r\|^2 = \|b - Ax\|^2 = \sum_{i=1}^{m} \left( b_i - \sum_{j=1}^{n} a_{ij}x_j \right)^2 \to \min_x
$$

## Решение через нормальные уравнения

Для нахождения минимума функции $S(x)$ необходимо приравнять её [[Градиент]] по $x$ к нулю.
$$
\nabla_x S(x) = \nabla_x (b - Ax)^T(b - Ax) = 0
$$
После дифференцирования получаем систему так называемых **нормальных уравнений**:
$$
A^T A x = A^T b
$$
Если матрица $A^T A$ невырождена (что эквивалентно линейной независимости столбцов матрицы $A$), то решение единственно и находится по формуле:
$$
x = (A^T A)^{-1} A^T b
$$
Матрица $(A^T A)^{-1} A^T$ называется **псевдообратной матрицей Мура-Пенроуза**.

**Проблема:** Прямое вычисление $A^T A$ может приводить к проблемам с численной устойчивостью, особенно если столбцы матрицы $A$ почти линейно зависимы. Число обусловленности матрицы $A^T A$ равно квадрату числа обусловленности $A$, что усугубляет ошибки вычислений.

## Решение через QR-разложение

Более численно устойчивый способ решения задачи МНК — использование [[QR-разложение|QR-разложения]] матрицы $A = QR$.

Подставляя это в исходную задачу минимизации, получаем:
$$
\|b - QRx\|^2 = \|Q^T(b - QRx)\|^2 = \|Q^T b - Rx\|^2 \to \min_x
$$
Здесь мы воспользовались тем, что умножение на ортогональную матрицу $Q^T$ сохраняет евклидову норму.

Задача сводится к решению системы $Rx = Q^T b$ с верхней треугольной матрицей $R$, которая легко решается **обратным ходом**. Этот подход позволяет избежать вычисления $A^T A$ и является предпочтительным на практике.

## Геометрическая интерпретация

Решение МНК $x$ даёт такой вектор $Ax$, который является **ортогональной проекцией** вектора $b$ на подпространство, натянутое на столбцы матрицы $A$. Вектор невязки $r = b - Ax$ при этом ортогонален этому подпространству.

## Пример на Python (линейная регрессия)

Рассмотрим задачу аппроксимации набора точек $(x_i, y_i)$ прямой $y = c_1 x + c_2$.

```python
import numpy as np
import matplotlib.pyplot as plt

def linear_regression_ols(x_data, y_data):
    """
    Выполняет линейную регрессию y = c1*x + c2 с помощью МНК.

    Args:
        x_data (np.ndarray): Вектор значений x.
        y_data (np.ndarray): Вектор значений y.

    Returns:
        np.ndarray: Вектор коэффициентов [c1, c2].
    """
    # Формируем матрицу A (матрицу плана)
    # Первый столбец - x_data, второй - единицы (для свободного члена c2)
    A = np.vstack([x_data, np.ones(len(x_data))]).T
    
    # Решаем задачу МНК с помощью встроенной функции numpy,
    # которая использует QR-разложение или SVD-разложение для устойчивости.
    c, _, _, _ = np.linalg.lstsq(A, y_data, rcond=None)
    
    return c

if __name__ == '__main__':
    # Генерируем случайные данные для примера
    np.random.seed(42)
    x_true = np.linspace(0, 10, 100)
    y_true = 2.5 * x_true + 1.5
    # Добавляем шум
    y_noisy = y_true + np.random.normal(0, 2, 100)

    # Находим коэффициенты
    c1, c2 = linear_regression_ols(x_true, y_noisy)

    print(f"Исходные коэффициенты: c1=2.5, c2=1.5")
    print(f"Найденные коэффициенты: c1={c1:.4f}, c2={c2:.4f}")

    # Строим график
    plt.figure(figsize=(10, 6))
    plt.scatter(x_true, y_noisy, label='Данные с шумом', s=10)
    plt.plot(x_true, y_true, color='green', linewidth=2, label='Истинная зависимость')
    plt.plot(x_true, c1 * x_true + c2, color='red', linewidth=2, label='Аппроксимация МНК')
    plt.title('Метод наименьших квадратов для линейной регрессии')
    plt.xlabel('x')
    plt.ylabel('y')
    plt.legend()
    plt.grid(True)
    plt.show()

```