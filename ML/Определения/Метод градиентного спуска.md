---
aliases:
  - метод градиентного спуска
  - gradient descent
---
**Метод градиентного спуска** — это итерационный метод оптимизации первого порядка для нахождения локального минимума дифференцируемой функции. Идея метода заключается в движении в направлении, противоположном [[Градиент|градиенту]] функции, так как [[Градиент|градиент]] указывает на направление наискорейшего роста.

## Основная идея

На каждом шаге алгоритм вычисляет [[Градиент|градиент]] $\nabla f(x)$ в текущей точке $x^{(k)}$ и делает шаг в направлении антиградиента (направления наискорейшего убывания функции).

Итерационный процесс описывается формулой:
$$
x^{(k+1)} = x^{(k)} - \lambda_k \nabla f(x^{(k)})
$$
где:
*   $x^{(k)}$ — текущее приближение к точке минимума.
*   $\nabla f(x^{(k)})$ — [[Градиент|градиент]] функции $f$ в точке $x^{(k)}$.
*   $\lambda_k > 0$ — **длина шага** (или **скорость обучения**, learning rate), которая определяет, насколько сильно мы сдвигаемся в направлении антиградиента.

Процесс повторяется до тех пор, пока не будет выполнен критерий остановки (например, норма [[Градиент|градиента]] станет достаточно малой или будет достигнуто максимальное число итераций).

## Алгоритм

### Входные данные
*   Дифференцируемая функция $f(x)$, которую нужно минимизировать.
*   Начальная точка $x^{(0)}$.
*   Скорость обучения $\lambda$.
*   Точность $\varepsilon$ для критерия остановки.

### Псевдокод

```
k = 0
x_current = x_initial

while ||∇f(x_current)|| > ε:
    x_next = x_current - λ * ∇f(x_current)
    x_current = x_next
    k = k + 1

return x_current
```

## Выбор длины шага ($\lambda$)

Выбор $\lambda$ критически важен для сходимости метода:
*   **Слишком маленькое $\lambda$**: Сходимость будет очень медленной.
*   **Слишком большое $\lambda$**: Метод может "перепрыгивать" через минимум и даже расходиться.

Существуют различные стратегии выбора шага:
1.  **Постоянный шаг**: $\lambda_k = \text{const}$. Самый простой подход, но требует подбора константы.
2.  **Дробление шага (Line Search)**: На каждой итерации решается задача одномерной минимизации для нахождения оптимального $\lambda_k$:
   $$
   \lambda_k = \arg\min_{\lambda > 0} f(x^{(k)} - \lambda \nabla f(x^{(k)}))
   $$

## Пример

Минимизируем функцию $f(x_1, x_2) = x_1^2 + 2x_2^2$.
Минимум очевиден: $(0, 0)$.

1.  **[[Градиент|Градиент]]**: $\nabla f(x) = (2x_1, 4x_2)^T$.
2.  **Начальная точка**: $x^{(0)} = (2, 1)^T$.
3.  **Длина шага**: $\lambda = 0.2$.

**Итерация 1:**
*   Вычисляем [[Градиент|градиент]] в $x^{(0)}$: $\nabla f(2, 1) = (2 \cdot 2, 4 \cdot 1)^T = (4, 4)^T$.
*   Делаем шаг:
   $$
   x^{(1)} = \begin{pmatrix} 2 \\ 1 \end{pmatrix} - 0.2 \begin{pmatrix} 4 \\ 4 \end{pmatrix} = \begin{pmatrix} 2 - 0.8 \\ 1 - 0.8 \end{pmatrix} = \begin{pmatrix} 1.2 \\ 0.2 \end{pmatrix}
   $$

**Итерация 2:**
*   Вычисляем [[Градиент|градиент]] в $x^{(1)}$: $\nabla f(1.2, 0.2) = (2 \cdot 1.2, 4 \cdot 0.2)^T = (2.4, 0.8)^T$.
*   Делаем шаг:
   $$
   x^{(2)} = \begin{pmatrix} 1.2 \\ 0.2 \end{pmatrix} - 0.2 \begin{pmatrix} 2.4 \\ 0.8 \end{pmatrix} = \begin{pmatrix} 1.2 - 0.48 \\ 0.2 - 0.16 \end{pmatrix} = \begin{pmatrix} 0.72 \\ 0.04 \end{pmatrix}
   $$

С каждой итерацией точка $x^{(k)}$ приближается к минимуму $(0, 0)$.

## Сравнение с [[Метод Ньютона|методом Ньютона]]

*   **Скорость сходимости**: Градиентный спуск имеет **линейную** скорость сходимости, что медленнее **квадратичной** сходимости [[Метод Ньютона|метода Ньютона]].
*   **Вычислительная сложность**: Каждая итерация градиентного спуска требует вычисления только [[Градиент|градиента]] (первой производной), что делает его вычислительно дешевле [[Метод Ньютона|метода Ньютона]], который требует вычисления и обращения [[Матрица Гессе|матрицы Гессе]] (вторых производных).
*   **Применение**: Благодаря простоте и меньшим требованиям к памяти, градиентный спуск и его стохастические варианты ([[Стохастический градиентный спуск (SGD)|SGD]], [[Метод Adam|Adam]]) являются основными методами обучения в глубоком обучении (Deep Learning).

