---
aliases:
  - EM-алгоритм
  - Expectation-Maximization
  - алгоритм ожидания-максимизации
---
**EM-алгоритм (Expectation-Maximization, алгоритм ожидания-максимизации)** — это итерационный метод для нахождения оценок [[Метод максимального правдоподобия|максимального правдоподобия]] параметров вероятностных моделей в случаях, когда модель зависит от некоторых **скрытых (латентных) переменных**.

Он особенно полезен, когда прямая максимизация функции правдоподобия затруднительна, но была бы проста, если бы значения скрытых переменных были известны.

## Основная идея

Проблема прямой оптимизации правдоподобия $p(X|\theta)$ в моделях со скрытыми переменными $Z$ заключается в том, что логарифм правдоподобия содержит сумму под знаком логарифма, что усложняет вычисление производных:
$$
\mathcal{L}(\theta | X) = \ln p(X|\theta) = \ln \sum_{Z} p(X, Z | \theta)
$$
EM-алгоритм решает эту проблему, итерационно чередуя два шага:

1.  **E-шаг (Expectation / Ожидание)**: На этом шаге, имея текущие оценки параметров $\theta^{(t)}$, мы вычисляем "мягкие" значения скрытых переменных. Формально, мы вычисляем апостериорное [[Распределение|распределение]] скрытых переменных $p(Z|X, \theta^{(t)})$ и на его основе строим функцию $Q(\theta|\theta^{(t)})$ — математическое ожидание полного логарифма правдоподобия $\ln p(X, Z | \theta)$.

2.  **M-шаг (Maximization / Максимизация)**: На этом шаге мы находим новые значения параметров $\theta^{(t+1)}$, которые максимизируют функцию $Q$, построенную на E-шаге. Эта задача максимизации, как правило, значительно проще исходной.
    $$
    \theta^{(t+1)} = \arg\max_{\theta} Q(\theta|\theta^{(t)})
    $$

Эти два шага повторяются до сходимости (например, пока правдоподобие не перестанет значительно увеличиваться).

## Алгоритм

1.  **Инициализация**: Выбрать начальные значения параметров $\theta^{(0)}$.
2.  **Итерационный процесс**: Повторять для $t = 0, 1, 2, \dots$ до сходимости:
    *   **E-шаг**: Вычислить функцию $Q(\theta|\theta^{(t)})$:
        $$
        Q(\theta|\theta^{(t)}) = E_{Z|X, \theta^{(t)}}[\ln p(X, Z | \theta)] = \sum_{Z} p(Z|X, \theta^{(t)}) \ln p(X, Z | \theta)
        $$
    *   **M-шаг**: Найти новые параметры, максимизируя $Q$:
        $$
        \theta^{(t+1)} = \arg\max_{\theta} Q(\theta|\theta^{(t)})
        $$

## Пример: Кластеризация на основе [[Смеси распределений|смеси гауссиан]] (GMM)

Это классический пример применения EM-алгоритма.

*   **Задача**: Разделить набор данных $X = \{x_1, \dots, x_n\}$ на $K$ кластеров.
*   **Модель**: Данные описываются [[Смеси распределений|смесью]] $K$ гауссовских распределений с параметрами $\theta = \{\pi_k, \mu_k, \Sigma_k\}_{k=1}^K$.
*   **Скрытые переменные**: Для каждой точки $x_i$ есть скрытая переменная $z_i \in \{1, \dots, K\}$, указывающая, к какому кластеру (гауссиане) она принадлежит.

**Алгоритм для GMM:**

1.  **Инициализация**: Задать начальные значения для $\pi_k, \mu_k, \Sigma_k$.

2.  **E-шаг**: Для каждой точки $x_i$ и каждого кластера $k$ вычислить "ответственность" (responsibility) $\gamma_{ik}$ — апостериорную вероятность того, что точка $x_i$ принадлежит кластеру $k$:
    $$
    \gamma_{ik} = p(z_i=k | x_i, \theta^{(t)}) = \frac{\pi_k^{(t)} \mathcal{N}(x_i | \mu_k^{(t)}, \Sigma_k^{(t)})}{\sum_{j=1}^{K} \pi_j^{(t)} \mathcal{N}(x_i | \mu_j^{(t)}, \Sigma_j^{(t)})}
    $$

3.  **M-шаг**: Пересчитать параметры модели, используя вычисленные ответственности в качестве весов:
    *   Новые веса компонент: $\pi_k^{(t+1)} = \frac{1}{n} \sum_{i=1}^{n} \gamma_{ik}$
    *   Новые центры кластеров: $\mu_k^{(t+1)} = \frac{\sum_{i=1}^{n} \gamma_{ik} x_i}{\sum_{i=1}^{n} \gamma_{ik}}$
    *   Новые ковариационные матрицы: $\Sigma_k^{(t+1)} = \frac{\sum_{i=1}^{n} \gamma_{ik} (x_i - \mu_k^{(t+1)})(x_i - \mu_k^{(t+1)})^T}{\sum_{i=1}^{n} \gamma_{ik}}$

4.  **Повторение**: Повторять шаги 2 и 3 до сходимости.

## Свойства

*   **Гарантированная сходимость**: EM-алгоритм гарантирует, что на каждой итерации значение функции правдоподобия не уменьшается: $\mathcal{L}(\theta^{(t+1)}) \ge \mathcal{L}(\theta^{(t)})$.
*   **Локальный [[Необходимые условия максимума|максимум]]**: Алгоритм сходится к локальному максимуму функции правдоподобия. Результат может зависеть от начальной инициализации параметров.
*   **Широкое применение**: Помимо GMM, EM-алгоритм используется в моделях скрытых марковских цепей ([[Скрытая марковская модель|HMM]]), в задачах с пропущенными данными и во многих других областях статистики и машинного обучения.