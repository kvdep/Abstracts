---
aliases:
  - метод релаксации
  - метод последовательной верхней релаксации
  - SOR
  - Successive Over-Relaxation
---
# Метод релаксации (SOR)

**Метод релаксации**, или **метод последовательной верхней релаксации (Successive Over-Relaxation, SOR)**, — это итерационный численный метод для решения [[Система линейных алгебраических уравнений|системы линейных алгебраических уравнений (СЛАУ)]]. Он является обобщением [[Метод Зейделя|метода Зейделя]] и часто позволяет значительно ускорить сходимость.

## Основная идея

[[Метод Зейделя]] на каждой итерации вычисляет новую компоненту вектора $x_i^{(k+1)}$ и сразу же использует её. Метод релаксации делает то же самое, но следующий шаг вычисляется как "взвешенная" сумма значения с предыдущей итерации $x_i^{(k)}$ и нового значения, полученного по формуле Зейделя.

Степень этого "взвешивания" регулируется **параметром релаксации** $\omega$.

### Параметр релаксации $\omega$

Выбор параметра $\omega$ критически важен для эффективности метода:
*   **$0 < \omega < 1$**: **Нижняя релаксация (Under-relaxation)**. Этот режим используется, когда [[Метод Зейделя|метод Зейделя]] расходится. Введение $\omega < 1$ может "замедлить" итерационный процесс и заставить его сойтись.
*   **$\omega = 1$**: Метод полностью совпадает с **[[Метод Зейделя|методом Зейделя]]**.
*   **$1 < \omega < 2$**: **Верхняя релаксация (Over-relaxation)**. Этот режим используется для ускорения сходимости уже сходящегося [[Метод Зейделя|метода Зейделя]]. Это наиболее частый случай применения метода, поэтому его часто называют SOR.

## Итерационная формула

На $(k+1)$-й итерации для каждой компоненты $i = 1, 2, \dots, n$ сначала вычисляется "зейделевское" приближение:
$$
\tilde{x}_i^{(k+1)} = \frac{1}{a_{ii}} \left( b_i - \sum_{j=1}^{i-1} a_{ij}x_j^{(k+1)} - \sum_{j=i+1}^{n} a_{ij}x_j^{(k)} \right)
$$

Затем итоговое значение $x_i^{(k+1)}$ вычисляется как линейная комбинация старого значения $x_i^{(k)}$ и нового $\tilde{x}_i^{(k+1)}$:
$$
x_i^{(k+1)} = (1 - \omega) x_i^{(k)} + \omega \tilde{x}_i^{(k+1)}
$$

Объединив эти две формулы, получаем полную итерационную формулу метода релаксации:
$$
x_i^{(k+1)} = (1 - \omega) x_i^{(k)} + \frac{\omega}{a_{ii}} \left( b_i - \sum_{j=1}^{i-1} a_{ij}x_j^{(k+1)} - \sum_{j=i+1}^{n} a_{ij}x_j^{(k)} \right)
$$

## Условия сходимости

*   **Необходимое условие**: для сходимости метода необходимо, чтобы параметр релаксации лежал в интервале $0 < \omega < 2$.
*   **Достаточное условие**: если матрица $A$ системы $Ax=b$ является **симметричной и положительно определённой**, то метод сходится для любого $\omega \in (0, 2)$.

Нахождение **оптимального значения $\omega$** — сложная задача, но даже приближённый подбор $\omega > 1$ может дать существенное ускорение по сравнению с методом Зейделя.

## Пример реализации на Python (для СЛАУ)

```python
import numpy as np

def sor_method(A, b, omega, x0=None, tol=1e-6, max_iter=1000):
    """
    Реализация метода последовательной верхней релаксации (SOR) для решения СЛАУ Ax = b.

    Args:
        A (np.ndarray): Матрица коэффициентов (n x n).
        b (np.ndarray): Вектор свободных членов (n x 1).
        omega (float): Параметр релаксации (0 < omega < 2).
        x0 (np.ndarray, optional): Начальное приближение. Если None, используется нулевой вектор.
        tol (float): Точность решения.
        max_iter (int): Максимальное количество итераций.

    Returns:
        (np.ndarray, int): Кортеж (вектор решения, количество итераций).
                           Возвращает (None, -1) если метод не сошелся.
    """
    n = len(b)
    x = np.zeros(n) if x0 is None else np.copy(x0)

    for k in range(max_iter):
        x_old = np.copy(x)
        
        for i in range(n):
            # Сумма с уже обновленными компонентами x_j^{(k+1)}
            sum1 = np.dot(A[i, :i], x[:i])
            # Сумма с компонентами с прошлой итерации x_j^{(k)}
            sum2 = np.dot(A[i, i + 1:], x_old[i + 1:])
            
            if A[i, i] == 0:
                print("Ошибка: Нулевой диагональный элемент.")
                return None, -1

            # Формула метода релаксации
            x[i] = (1 - omega) * x_old[i] + (omega / A[i, i]) * (b[i] - sum1 - sum2)

        # Проверка на сходимость
        if np.linalg.norm(x - x_old, ord=np.inf) < tol:
            return x, k + 1

    print(f"Метод (omega={omega}) не сошелся за максимальное количество итераций.")
    return x, max_iter

if __name__ == '__main__':
    A = np.array([[10, -1, 2, 0],
                  [-1, 11, -1, 3],
                  [2, -1, 10, -1],
                  [0, 3, -1, 8]], dtype=float)
    b = np.array([6, 25, -11, 15], dtype=float)

    # Сравним сходимость для разных omega
    
    # omega = 1 (эквивалентно методу Зейделя)
    solution_seidel, iterations_seidel = sor_method(A, b, omega=1.0)
    if solution_seidel is not None:
        print(f"Решение для omega=1.0 (Зейдель) найдено за {iterations_seidel} итераций.")
        print(solution_seidel)

    print("-" * 30)

    # omega > 1 (верхняя релаксация)
    # Оптимальное значение для этой матрицы около 1.1
    solution_sor, iterations_sor = sor_method(A, b, omega=1.1)
    if solution_sor is not None:
        print(f"Решение для omega=1.1 (SOR) найдено за {iterations_sor} итераций:")
        print(solution_sor)
        print("\nПроверка A*x:")
        print(np.dot(A, solution_sor))

```