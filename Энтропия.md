---
aliases:
  - Энтропия
  - Энтропия Шеннона
  - Информационная энтропия
  - Shannon Entropy
  - Information Entropy
---
### Информационная энтропия (Энтропия Шеннона)

Говорят, что **информационная энтропия** (или **энтропия Шеннона**) — это мера неопределенности, присущей случайной величине или, в более прикладном смысле, "перемешанности" набора данных. В машинном обучении она является ключевым критерием, используемым в алгоритмах построения **деревьев решений** (таких как ID3, C4.5), для оценки качества разделения данных.

Простыми словами, энтропия показывает, какое количество информации (в битах) в среднем требуется для кодирования сообщения о том, к какому классу принадлежит случайно выбранный элемент из набора. Чем выше энтропия, тем больше неопределенность.

#### Основная идея

*   **Чистый узел (Pure Node):** Если все элементы в узле принадлежат одному классу, неопределенность минимальна, и энтропия равна нулю. Для кодирования информации о классе не требуется никаких данных — он и так известен.
*   **Нечистый узел (Impure Node):** Если элементы в узле равномерно распределены по разным классам, неопределенность максимальна, и энтропия достигает своего пика.

Цель алгоритма построения дерева решений — найти такие разделения, которые приводят к максимальному снижению энтропии, то есть к максимальному **приросту информации (Information Gain)**.

#### Формула

Энтропия для одного узла рассчитывается по формуле Шеннона:

$$
H(X) = - \sum_{i=1}^{C} p_i \log_2(p_i)
$$

где:
*   $C$ — количество классов в наборе данных.
*   $p_i$ — доля (вероятность) элементов, принадлежащих классу $i$ в данном узле.
*   $\log_2$ — логарифм по основанию 2. Использование этого основания означает, что энтропия измеряется в **битах**.
*   По определению, слагаемое $p_i \log_2(p_i)$ считается равным нулю, если $p_i = 0$.

Знак "минус" в начале формулы нужен для того, чтобы итоговое значение было неотрицательным, так как логарифм вероятности (числа от 0 до 1) всегда неположителен.

#### Пример расчета

Представим, что у нас есть узел с 10 элементами, которые нужно классифицировать на два класса: "Синий" и "Красный".

**Случай 1: Чистый узел**
*   10 "Синих", 0 "Красных".
*   $p_{синий} = \frac{10}{10} = 1$
*   $p_{красный} = \frac{0}{10} = 0$
*   $H = - (1 \cdot \log_2(1) + 0 \cdot \log_2(0)) = - (1 \cdot 0 + 0) = 0$
    Энтропия равна нулю. Неопределенности нет.

**Случай 2: Максимально нечистый узел**
*   5 "Синих", 5 "Красных".
*   $p_{синий} = \frac{5}{10} = 0.5$
*   $p_{красный} = \frac{5}{10} = 0.5$
*   $H = - (0.5 \cdot \log_2(0.5) + 0.5 \cdot \log_2(0.5)) = - (0.5 \cdot (-1) + 0.5 \cdot (-1)) = -(-0.5 - 0.5) = 1$
    Энтропия максимальна и равна 1 биту. Требуется 1 бит информации, чтобы указать класс.

**Случай 3: Смешанный узел**
*   7 "Синих", 3 "Красных".
*   $p_{синий} = \frac{7}{10} = 0.7$
*   $p_{красный} = \frac{3}{10} = 0.3$
*   $H = - (0.7 \cdot \log_2(0.7) + 0.3 \cdot \log_2(0.3)) \approx - (0.7 \cdot (-0.51) + 0.3 \cdot (-1.74)) \approx 0.88$
    Неопределенность существует, но она меньше, чем в предыдущем случае.

### Использование в деревьях решений: Прирост информации (Information Gain)

При построении дерева решений алгоритм выбирает разделение, которое дает максимальный **прирост информации (Information Gain)**. Он показывает, насколько уменьшилась энтропия после разделения узла на дочерние.

Формула для расчета взвешенной энтропии после разделения:
$$
H_{split} = \sum_{k=1}^{m} \frac{N_k}{N} H_k
$$

где:
*   $m$ — количество дочерних узлов.
*   $N$ — общее количество элементов в родительском узле.
*   $N_k$ — количество элементов в $k$-м дочернем узле.
*   $H_k$ — энтропия $k$-го дочернего узла.

**Information Gain** вычисляется как разница между энтропией родительского узла и взвешенной энтропией дочерних узлов:
$$
\text{Information Gain} = H_{parent} - H_{split}
$$
Алгоритм выбирает то разделение, у которого `Information Gain` максимален.

### Сравнение с критерием Джини

Энтропия часто сравнивается с другим популярным критерием — **[[Gini|критерием неопределенности Джини]]**.

| Критерий     | Преимущества                                                                                                                                                           | Недостатки                                                |
| :----------- | :--------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :-------------------------------------------------------- |
| **Энтропия** | - Более чувствителен к распределению классов, что иногда дает более сбалансированные деревья. <br> - Имеет строгое теоретическое обоснование в теории информации.      | - Вычислительно сложнее из-за логарифмов.                 |
| [[Gini]]     | - Вычислительно проще и быстрее, так как не содержит логарифмов. <br> - На практике дает очень похожие результаты. <br> - Стремится изолировать наиболее частый класс. | - Менее чувствителен к изменениям в вероятностях классов. |

На практике разница в итоговой точности моделей, построенных с использованием энтропии и [[Gini|индекса Джини]], как правило, незначительна. Из-за вычислительной простоты [[Gini|индекс Джини]] часто используется по умолчанию в таких библиотеках, как Scikit-learn.