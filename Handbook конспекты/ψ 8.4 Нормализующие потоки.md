#Handbook
Выразительность функции <=> Expressive Power <=> способность функции представлять сложные зависимости.

Нормализующий поток <=> $f_\theta$
Нормализующий потому что мы переводим [[Распределение|распределение]] из X в [[Распределение|распределение]] $N(0,I)$ в Z (f такое отображение, что сэмплы x составят в пространстве z нормальное [[Распределение|распределение]])

$p_x(x) = p_z(f^{-1}(x)) \cdot |det(J_{f^{-1}})|$ - ф. правдоподобия
Смысл Якобиана: $\Delta y$ ~ $\Delta x \cdot |J|$, т.е. связываем изменение в одном пространстве с изменением в другом пространстве. Так можно считать плотность в неизвестном вероятностном пространстве в известном вероятностном пространстве, зная якобиан. 
Логарифмируем 


Запишем f как композицию:
$f = f_1 ∘ f_2 ∘ ... ∘f_K$
$det(J_{f^{-1}}) = \prod^K_{i=1} det(J_{f_i^-1})$ 

Считать Якобианы сложно, поэтому нам нужны отображения, которые считаются быстро

Planar flow <=> планарный поток <=> $f_\theta(z) = z+ u_\theta h (w_\theta^T z + b_\theta)$
$u_\theta, w_\theta, b_\theta - \text{обучаемые параметры}$
$h - \text{нелинейная фунцкия}$ 
Якобиан: $|det(J_f)| = |det(\mathbb{1} + u_\theta \psi(z)^T)| = |(1 + u_\theta^T\psi(z))|$, считается за $O(n)$

Детерминант у треугольных матриц считается быстро; найдем такие функции, якобиан которых - треугольная матрица.


NICE <=> Non-linear Independent Component Estimation ; 

Аддитивное связывание <=> additive coupling
$x = f_\theta(z) = \begin{cases} x_{1:d} = z_{1:d} \\ x_{d+1:n} = z_{d+1:n} + m_\theta(z_{1:d}) \end{cases}$
$1 < d < n$
$m_\theta$ - произвольная нейросеть с d входами и $n-d$ выходами
Якобиан = 1
Смысл: вектор x такой же как z с 1 по d, дальше на него влияет модель $m_\theta$. Т.к. изменяется только часть x, остальная часть имеет производную по z равную 0.


RealNVP 

Аффинное связывание <=> affine coupling 
$x = f_\theta(z) = \begin{cases} x_{1:d} = z_{1:d} \\ x_{d+1:n} = exp(s_\theta (z_{1:d})) \odot z_{d+1:n} + m_\theta(z_{1:d}) \end{cases}$
$\odot$ - поэлементное умножение
$s_\theta$ - нейросеть, обычно с архитектурой $m_\theta$
Якобиан:
$det(J_{f-1}) = exp \sum^n_{i=d+1} (s_\theta(z_{1:d}))_i$
Экспонента нужна для масштабирования элементов (аналогия scale и shift)



Masked Autoregressive Flows
$x_i = z_i \exp{f_{\alpha_i}(x_{1:i-1})} + f_{\mu_i}(x_{1:i-1})$
$\mu,\alpha$ - нейросети
Обладает нижнетреугольным Якобианом
$det(J_{f^{-1}}) = exp(-\sum^n_{i=1} f_{\alpha_i}(x_{1:i-1}))$
Не подходит для высокоразмерных данных из-за авторегрессивной части - чем длиннее последовательность, тем дольше работа.

Inverse Autoregressive Flows
Хотим ускорить генерацию $x_i$ убрав авторегрессию у $x$
$z_i = (x_i - f_{\mu_i}(z_{1:i-1})) exp(-f_{\alpha_i}(z_{1:i-1}))$
$x_i = z_i \cdot exp (f_{\alpha_i}(z_{1:i-1})) + f_{\mu_i}(z_{1:i-1})$
Здесь авторегрессия в вычислении z, из-за него обучение долгое.


Звук и нормализующие потоки
Основной смысл: генерируем mel-спектрограмму (картинку), затем используем её для генерации аудиоволны (waveform)
Почему: waveform сразу генерировать очень тяжело - если аудио в 16 HZ, то это 16000 измерений в секунду. Вместо этого можно сгенерить картинку спектрограммы, выделяющую важные для человеческого слуха частоты (mel-спектрограмму) (остальные частоты на картинке будут подавлены из-за маленького масштаба), и потом на основе неё генерить waveform как по чертежу
