Выразительность функции <=> Expressive Power <=> способность функции представлять сложные зависимости 

Нормализующий поток <=> $f_\theta$

$p_x(x) = p_z(f^{-1}(x)) \cdot |det(J_{f^{-1}})|$ - ф. правдоподобия
Логарифмируем

Запишем f как композицию:
$f = f_1 ∘ f_2 ∘ ... ∘f_K$
$det(J_{f^{-1}}) = \prod^K_{i=1} det(J_{f_i^-1})$ 

Считать Якобианы сложно, поэтому нам нужны отображения, которые считаются быстро

Planar flow <=> планарный поток <=> $f_\theta(z) = z+ u_\theta h (w_\theta^T z + b_\theta), u_\theta, w_\theta, b_\theta - \text{обучаемые параметры}, h - \text{нелинейная фунцкия}$ 
Якобиан: $|det(J_f)| = |det(\mathbb{1} + u_\theta \psi(z)^T)| = |(1 + u_\theta^T\psi(z))|$, считается за $O(n)$

Детерминант у треугольных матриц считается быстро; найдем такие функции, якобиан которых - треугольная матрица.


NICE <=> Non-linear Independent Component Estimation ; 

Аддитивное связывание <=> additive coupling
$x = f_\theta(z) = \begin{cases} x_{1:d} = z_{1:d} \\ x_{d+1:n} = z_{d+1:n} + m_\theta(z_{1:d}) \end{cases}$
$1 < d < n$
$m_\theta$ - произвольная нейросеть с d входами и $n-d$ выходами
Якобиан = 1
Смысл: вектор x такой же как z с 1 по d, дальше на него влияет модель $m_\theta$. Т.к. изменяется только часть x, остальная часть имеет производную по z равную 0.


RealNVP 

Аффинное связывание <=> affine coupling 
$x = f_\theta(z) = \begin{cases} x_{1:d} = z_{1:d} \\ x_{d+1:n} = exp(s_\theta (z_{1:d})) \odot z_{d+1:n} + m_\theta(z_{1:d}) \end{cases}$
$\odot$ - поэлементное умножение
$s_\theta$ - нейросеть, обычно с архитектурой $m_\theta$
Якобиан:
$det(J_{f-1}) = exp \sum^n_{i=d+1} (s_\theta(z_{1:d}))_i$
Экспонента нужна для масштабирования элементов (аналогия scale и shift)



